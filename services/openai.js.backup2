require('dotenv').config();
const OpenAI = require('openai');
const fs = require('fs');
const path = require('path');
const GoogleService = require('./google');

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

class OpenAIService {
  constructor() {
    this.model = 'gpt-4';
    this.visionModel = 'gpt-4o'; // Updated from deprecated gpt-4-vision-preview
  }

  async chatCompletion(messages, conversationHistory = [], businessTone = null, businessId = null) {
    try {
      // Check if the latest message contains an email request
      const latestMessage = messages[messages.length - 1];
      const emailRequest = this.detectEmailRequest(latestMessage.content);
      const emailReadRequest = this.detectEmailReadRequest(latestMessage.content);
      
      // Handle email sending request
      if (emailRequest && businessId) {
        try {
          const result = await GoogleService.sendEmail(businessId, {
            to: emailRequest.to,
            subject: emailRequest.subject,
            body: emailRequest.body
          });
          
          return `‚úÖ Email sent successfully to ${emailRequest.to}!\n\nSubject: ${emailRequest.subject}\n\nMessage: ${emailRequest.body}`;
        } catch (error) {
          console.error('Error sending email:', error);
          return `‚ùå Sorry, I couldn't send the email. Please make sure Google Workspace integration is properly configured. Error: ${error.message}`;
        }
      }

      // Handle email reading request
      if (emailReadRequest && businessId) {
        try {
          let emails = [];
          let response = '';

          switch (emailReadRequest.type) {
            case 'unread':
              emails = await GoogleService.getUnreadEmails(businessId, emailReadRequest.maxResults || 5);
              response = `üìß Here are your unread emails (${emails.length} found):\n\n`;
              break;
            case 'recent':
              emails = await GoogleService.getEmails(businessId, { maxResults: emailReadRequest.maxResults || 5 });
              response = `üìß Here are your recent emails (${emails.length} found):\n\n`;
              break;
            case 'search':
              emails = await GoogleService.searchEmails(businessId, emailReadRequest.query, emailReadRequest.maxResults || 5);
              response = `üìß Search results for "${emailReadRequest.query}" (${emails.length} found):\n\n`;
              break;
            case 'label':
              emails = await GoogleService.getEmailsByLabel(businessId, emailReadRequest.label, emailReadRequest.maxResults || 5);
              response = `üìß Emails from "${emailReadRequest.label}" (${emails.length} found):\n\n`;
              break;
            default:
              emails = await GoogleService.getEmails(businessId, { maxResults: 5 });
              response = `üìß Here are your recent emails (${emails.length} found):\n\n`;
          }

          if (emails.length === 0) {
            return `üìß No emails found for your request.`;
          }

          // Format emails for display
          emails.forEach((email, index) => {
            const date = new Date(email.internalDate).toLocaleString();
            const isUnread = email.labelIds && email.labelIds.includes('UNREAD') ? 'üîµ ' : '';
            const attachmentInfo = email.attachments && email.attachments.length > 0 ? ` üìé (${email.attachments.length} attachments)` : '';
            
            response += `${index + 1}. ${isUnread}**${email.subject || 'No Subject'}**\n`;
            response += `   üì§ From: ${email.from || 'Unknown'}\n`;
            response += `   üìÖ Date: ${date}\n`;
            response += `   üí¨ Preview: ${(email.snippet || email.body || '').substring(0, 100)}...${attachmentInfo}\n\n`;
          });

          return response;
        } catch (error) {
          console.error('Error reading emails:', error);
          return `‚ùå Sorry, I couldn't read your emails. Please make sure Google Workspace integration is properly configured. Error: ${error.message}`;
        }
      }


      let systemContent = `You are a helpful AI assistant integrated with WhatsApp and Google Workspace. 
      You can send and read emails through Gmail when users request it. Be conversational, friendly, and helpful. 
      Keep responses concise but informative. If you're analyzing images, describe what you see clearly and provide relevant insights.
      
      When users ask to send emails, you can help them by sending emails through Gmail integration.
      Format for email sending: "send email to [email] with subject [subject] and body [body]"
      
      When users ask to read emails, you can help them access their Gmail. Examples:
      - "show me my unread emails" or "check unread emails"
      - "show me recent emails" or "get my latest emails"
      - "search emails for [query]" or "find emails about [topic]"
      - "show emails from [label]" (like Important, Promotions, etc.)`;
      // Apply business-specific tone if provided
      if (businessTone && businessTone.tone_instructions) {
        systemContent += `\n\n${businessTone.tone_instructions}`;
      }

      const systemMessage = {
        role: 'system',
        content: systemContent
      };

      const allMessages = [systemMessage, ...conversationHistory, ...messages];

      const response = await openai.chat.completions.create({
        model: this.model,
        messages: allMessages,
        max_tokens: 1000,
        temperature: 0.7,
      });

      return response.choices[0].message.content;
    } catch (error) {
      console.error('OpenAI chat completion error:', error);
      throw new Error('Failed to generate AI response');
    }
  }

  async analyzeImage(imagePath, userMessage = '', businessTone = null) {
    try {
      console.log(`OpenAI: Analyzing image at path: ${imagePath}`);
      
      // Check if file exists
      if (!fs.existsSync(imagePath)) {
        console.error(`OpenAI: Image file not found: ${imagePath}`);
        throw new Error(`Image file not found: ${imagePath}`);
      }

      const imageBuffer = fs.readFileSync(imagePath);
      const base64Image = imageBuffer.toString('base64');
      
      console.log(`OpenAI: Image file size: ${imageBuffer.length} bytes`);
      console.log(`OpenAI: Base64 length: ${base64Image.length} characters`);

      let promptText = 'Please analyze this image and describe what you see in detail. Include any text, objects, people, colors, or important details. Be specific and helpful in your description.';
      
      // If user sent a message with the image, include it in the analysis
      if (userMessage && userMessage.trim() !== '' && userMessage !== 'User sent a image message') {
        promptText += ` The user also sent this message with the image: "${userMessage}". Please consider this context in your analysis.`;
      }
      
      // Apply business-specific tone if provided
      if (businessTone && businessTone.tone_instructions) {
        promptText += `\n\n${businessTone.tone_instructions}`;
      }

      console.log(`OpenAI: Using prompt: ${promptText}`);

      const messages = [
        {
          role: 'user',
          content: [
            {
              type: 'text',
              text: promptText
            },
            {
              type: 'image_url',
              image_url: {
                url: `data:image/jpeg;base64,${base64Image}`,
              },
            },
          ],
        },
      ];

      console.log(`OpenAI: Sending request to OpenAI with model: ${this.visionModel}`);

      const response = await openai.chat.completions.create({
        model: this.visionModel,
        messages: messages,
        max_tokens: 1500,
        temperature: 0.7,
      });

      console.log(`OpenAI: Received response: ${response.choices[0].message.content}`);
      return response.choices[0].message.content;
    } catch (error) {
      console.error('OpenAI image analysis error:', error);
      console.error('OpenAI error details:', error.message);
      console.error('OpenAI error stack:', error.stack);
      throw new Error(`Failed to analyze image: ${error.message}`);
    }
  }

  async transcribeAudio(audioPath) {
    try {
      // Check if file exists
      if (!fs.existsSync(audioPath)) {
        throw new Error(`Audio file not found: ${audioPath}`);
      }

      const audioFile = fs.createReadStream(audioPath);
      
      const response = await openai.audio.transcriptions.create({
        file: audioFile,
        model: 'whisper-1',
        response_format: 'text',
        language: 'en',
        temperature: 0.2, // Lower temperature for more accurate transcription
      });

      return response;
    } catch (error) {
      console.error('OpenAI transcription error:', error);
      throw error;
    }
  }

  async processMessage(messageType, content, filePath = null, conversationHistory = [], businessTone = null, businessId = null) {
    try {
      console.log(`OpenAI: Processing message type: ${messageType}`);
      console.log(`OpenAI: Content: ${content}`);
      console.log(`OpenAI: File path: ${filePath}`);
      console.log(`OpenAI: File exists: ${filePath ? fs.existsSync(filePath) : 'N/A'}`);
      
      let aiResponse = '';

      switch (messageType) {
        case 'text':
          console.log('OpenAI: Processing text message');
          aiResponse = await this.chatCompletion([
            { role: 'user', content: content }
          ], conversationHistory, businessTone, businessId);
          break;

        case 'image':
          console.log('OpenAI: Processing image message');
          if (!filePath) {
            console.error('OpenAI: Image file path is required for image analysis');
            throw new Error('Image file path is required for image analysis');
          }
          if (!fs.existsSync(filePath)) {
            console.error(`OpenAI: Image file does not exist: ${filePath}`);
            throw new Error(`Image file does not exist: ${filePath}`);
          }
          // For images, analyze directly and provide a conversational response
          const imageAnalysis = await this.analyzeImage(filePath, content, businessTone);
          
          // If there's user text with the image, combine it with the analysis
          if (content && content.trim() !== '' && content !== `User sent a ${messageType} message`) {
            aiResponse = await this.chatCompletion([
              { role: 'user', content: `User sent an image with this message: "${content}". Here's what I see in the image: ${imageAnalysis}. Please respond to both the image and the user's message.` }
            ], conversationHistory, businessTone, businessId);
          } else {
            // Just respond to the image analysis
            aiResponse = await this.chatCompletion([
              { role: 'user', content: `I analyzed this image and here's what I see: ${imageAnalysis}. Please provide a helpful response about what's in the image.` }
            ], conversationHistory, businessTone, businessId);
          }
          break;

        case 'audio':
          console.log('OpenAI: Processing audio message');
          if (!filePath) {
            console.error('OpenAI: Audio file path is required for transcription');
            throw new Error('Audio file path is required for transcription');
          }
          if (!fs.existsSync(filePath)) {
            console.error(`OpenAI: Audio file does not exist: ${filePath}`);
            throw new Error(`Audio file does not exist: ${filePath}`);
          }
          const transcription = await this.transcribeAudio(filePath);
          aiResponse = await this.chatCompletion([
            { role: 'user', content: `Transcribed audio: "${transcription}". Please respond to this message naturally and conversationally.` }
          ], conversationHistory, businessTone, businessId);
          break;

        default:
          console.error(`OpenAI: Unsupported message type: ${messageType}`);
          throw new Error(`Unsupported message type: ${messageType}`);
      }

      console.log(`OpenAI: Generated response: ${aiResponse}`);
      return aiResponse;
    } catch (error) {
      console.error('OpenAI: Error processing message:', error);
      console.error('OpenAI: Error details:', error.message);
      console.error('OpenAI: Error stack:', error.stack);
      throw error;
    }
  }


  detectEmailRequest(message) {
    const emailRegex = /send\s+email\s+to\s+([^\s]+@[^\s]+)\s+with\s+subject\s+([^and]+?)\s+and\s+body\s+(.+)/i;
    const match = message.match(emailRegex);
    
    if (match) {
      return {
        to: match[1].trim(),
        subject: match[2].trim(),
        body: match[3].trim()
      };
    }
    
    return null;
  }


  detectEmailReadRequest(message) {
    const lowercaseMessage = message.toLowerCase();
    
    // Check for unread emails - more flexible patterns
    if (lowercaseMessage.includes('unread') && 
        (lowercaseMessage.includes('email') || lowercaseMessage.includes('message'))) {
      return { type: 'unread', maxResults: this.extractNumber(message) || 5 };
    }
    
    // Check for recent/latest emails - more flexible patterns
    if ((lowercaseMessage.includes('recent') || lowercaseMessage.includes('latest') || lowercaseMessage.includes('new')) && 
        (lowercaseMessage.includes('email') || lowercaseMessage.includes('message'))) {
      return { type: 'recent', maxResults: this.extractNumber(message) || 5 };
    }
    
    // Check for general email reading requests
    if ((lowercaseMessage.includes('read') || lowercaseMessage.includes('show') || lowercaseMessage.includes('get')) && 
        (lowercaseMessage.includes('email') || lowercaseMessage.includes('message'))) {
      // If it mentions unread, prioritize unread
      if (lowercaseMessage.includes('unread')) {
        return { type: 'unread', maxResults: this.extractNumber(message) || 5 };
      }
      // Otherwise, get recent emails
      return { type: 'recent', maxResults: this.extractNumber(message) || 5 };
    }
    
    // Check for email search
    const searchMatch = message.match(/search\s+(?:emails?|messages?)\s+for\s+(.+)|find\s+(?:emails?|messages?)\s+about\s+(.+)|(?:emails?|messages?)\s+about\s+(.+)/i);
    if (searchMatch) {
      const query = searchMatch[1] || searchMatch[2] || searchMatch[3];
      return { 
        type: 'search', 
        query: query.trim(),
        maxResults: this.extractNumber(message) || 5 
      };
    }
    
    // Check for emails by label
    const labelMatch = message.match(/emails?\s+from\s+(\w+)|show\s+(\w+)\s+emails?/i);
    if (labelMatch) {
      const label = labelMatch[1] || labelMatch[2];
      return { 
        type: 'label', 
        label: label.trim(),
        maxResults: this.extractNumber(message) || 5 
      };
    }
    
    return null;
  }

  extractNumber(message) {
    const numberMatch = message.match(/(\d+)/);
    return numberMatch ? parseInt(numberMatch[1]) : null;
  }
}

module.exports = new OpenAIService();
